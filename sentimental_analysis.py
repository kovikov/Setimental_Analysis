# -*- coding: utf-8 -*-
"""sentimental_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MTuIUN63zanGkwbSJtLvRSdITShn_tIA
"""

import pandas as pd
from google.colab import files

# Load all the spreadsheets
students = pd.read_csv("/content/Students.csv")
instructors = pd.read_csv("/content/Instructors.csv")
courses = pd.read_csv("/content/Courses.csv")
reviews = pd.read_csv("/content/Reviews.csv")

# Add sentiment label
reviews["sentiment_label"] = reviews["rating_score"].apply(
    lambda x: "Negative" if x <= 2 else "Neutral" if x == 3 else "Positive"
)

# Merge all tables
merged = reviews.merge(students, on="student_id", how="left")
merged = merged.merge(instructors, on="instructor_id", how="left")
merged = merged.merge(courses, on="course_id", how="left")

# Save merged table to Colab's file system
merged_csv_path = "/content/Merged_All_Data.csv"
merged.to_csv(merged_csv_path, index=False)

# Download the file to your computer
files.download(merged_csv_path)

# Load the merged dataset for deep cleaning
merged_df = pd.read_csv("/content/Merged_All_Data.csv")

merged_df.head()

"""#Step 1: Preview basic structure and summary"""

structure_info = merged_df.info()

null_summary = merged_df.isnull().sum()

null_summary

head_preview = merged_df.head()

head_preview

"""# 2 Step-by-step deep cleaning process"""

# Step 1: Convert review_date to datetime
merged_df["review_date"] = pd.to_datetime(merged_df["review_date"], errors='coerce')

# Step 2: Clean text fields
text_columns = ["review_text", "sentiment_label", "region", "device_type",
                "full_name", "country", "title", "category", "language", "level"]
for col in text_columns:
    merged_df[col] = merged_df[col].astype(str).str.strip().str.title()

# Step 3: Remove duplicates
merged_df = merged_df.drop_duplicates()

# Step 4: Optional - Remove negative response times or out-of-range ratings
merged_df = merged_df[merged_df["rating_score"].between(1, 5)]
merged_df = merged_df[merged_df["response_time"] >= 0]

# Step 5: Save cleaned file
cleaned_file_path = "/content/Cleaned_Merged_All_Data.csv"
merged_df.to_csv(cleaned_file_path, index=False)

# Step 6: Download the file to your computer
files.download(cleaned_file_path)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the cleaned data
df = pd.read_csv("/content/Cleaned_Merged_All_Data.csv")

# Set up the visual style
sns.set(style="whitegrid")
plt.figure(figsize=(12, 6));

# 1. Sentiment distribution
plt.figure(figsize=(8, 4))
sns.countplot(x='sentiment_label', data=df, palette='Set2')
plt.title("Sentiment Label Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Number of Reviews")
plt.show()

# 2. Top 10 instructors by number of reviews
plt.figure(figsize=(10, 5))
top_instructors = df['full_name'].value_counts().head(10)
sns.barplot(x=top_instructors.values, y=top_instructors.index, palette="Blues_d")
plt.title("Top 10 Instructors by Review Count")
plt.xlabel("Number of Reviews")
plt.ylabel("Instructor")
plt.show()

# 3. Average Rating vs Satisfaction Score
plt.figure(figsize=(10, 6))
sns.scatterplot(x='rating_score', y='satisfaction_score', hue='sentiment_label', data=df, alpha=0.7)
plt.title("Rating Score vs. Satisfaction Score")
plt.xlabel("Rating Score")
plt.ylabel("Satisfaction Score")
plt.show()

# 4. Average response time per instructor (Top 10)
plt.figure(figsize=(12, 5))
avg_response = df.groupby('full_name')['response_time'].mean().sort_values(ascending=False).head(10)
sns.barplot(x=avg_response.values, y=avg_response.index, palette="Reds")
plt.title("Top 10 Instructors by Average Response Time")
plt.xlabel("Avg Response Time (hours)")
plt.ylabel("Instructor")
plt.show();

# 5. Most common course categories
plt.figure(figsize=(10, 5))
category_counts = df['category'].value_counts().head(10)
sns.barplot(x=category_counts.values, y=category_counts.index, palette="Set1")
plt.title("Most Common Course Categories")
plt.xlabel("Count")
plt.ylabel("Category")
plt.show()

"""üìä Visualizations Included:
Sentiment distribution

Top instructors by review count

Rating vs satisfaction score (scatter)

Average response time per instructor

Most common course categories

This heatmap will show the relationships between numerical features such as:

rating_score

response_time

satisfaction_score

completed_courses

avg_response_time

duration_hours
"""

# Load the cleaned dataset
df = pd.read_csv("/content/Cleaned_Merged_All_Data.csv")

# Select only the numeric columns of interest
numeric_cols = [
    'rating_score',
    'response_time',
    'satisfaction_score',
    'completed_courses',
    'avg_response_time',
    'duration_hours'
]

# Calculate the correlation matrix
correlation_matrix = df[numeric_cols].corr()

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Heatmap of Key Numeric Features")
plt.show()

# Load your cleaned dataset
df = pd.read_csv("/content/Cleaned_Merged_All_Data.csv")

# Define the numeric columns of interest
numeric_cols = [
    'rating_score',
    'response_time',
    'satisfaction_score',
    'completed_courses',
    'avg_response_time',
    'duration_hours'
]

# Get unique sentiment labels
sentiments = df['sentiment_label'].unique()

# Create a heatmap for each sentiment
for sentiment in sentiments:
    subset = df[df['sentiment_label'] == sentiment]
    corr_matrix = subset[numeric_cols].corr()

    # Plot
    plt.figure(figsize=(10, 6))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
    plt.title(f"Correlation Heatmap - {sentiment} Sentiment")
    plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer

# Load the data
df = pd.read_csv("/content/Cleaned_Merged_All_Data.csv")

# Drop NaNs just in case
df = df.dropna(subset=['review_text', 'sentiment_label'])

# Step 1: Extract keywords using CountVectorizer
vectorizer = CountVectorizer(stop_words='english', max_features=20)
X = vectorizer.fit_transform(df['review_text'])

# Step 2: Convert to DataFrame and add sentiment labels
keywords_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
keywords_df['sentiment_label'] = df['sentiment_label'].values

# Step 3: Group by sentiment and sum
grouped = keywords_df.groupby('sentiment_label').sum().T

# Step 4: Plot the grouped bar chart
plt.figure(figsize=(14, 7))
grouped.plot(kind='bar', figsize=(14, 7), colormap='Set2')
plt.title("Top Keyword Frequency by Sentiment")
plt.xlabel("Keyword")
plt.ylabel("Frequency")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""üìä What This Shows:
The top 20 words found in student reviews

How often each is used in positive, neutral, and negative reviews

A side-by-side comparison for each keyword

Preprocess and clean review_text

Vectorize the text using TF-IDF

Optionally include metadata (e.g., course category, instructor_id)

Train a Logistic Regression model

Evaluate with accuracy, precision, recall

Predict sentiment for new reviews
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("/content/Cleaned_Merged_All_Data.csv")

# Drop rows with missing review text or sentiment
df = df.dropna(subset=["review_text", "sentiment_label"])

# Optional: encode metadata (e.g., instructor_id and category)
df["instructor_id"] = df["instructor_id"].astype(str)
df["category"] = df["category"].astype(str)

# Combine review text + metadata (optional)
df["full_text"] = df["review_text"] + " category_" + df["category"] + " instructor_" + df["instructor_id"]

# Encode target labels
le = LabelEncoder()
df["sentiment_encoded"] = le.fit_transform(df["sentiment_label"])  # Negative=0, Neutral=1, Positive=2

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(df["full_text"], df["sentiment_encoded"], test_size=0.2, random_state=42)

# TF-IDF + Logistic Regression pipeline
model = make_pipeline(
    TfidfVectorizer(stop_words="english", max_features=3000),
    LogisticRegression(max_iter=200)
)

# Train model
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=le.classes_))

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=le.classes_, yticklabels=le.classes_, cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True Sentiment")
plt.title("Confusion Matrix")
plt.show()

"""i ambuilding a sentiment prediction pipeline, and you want to compare multiple ML models to see which one performs best.

Here‚Äôs a full, organized code in Google Colab that:

Uses TF-IDF to vectorize review_text

Trains and compares 4 model candidates:

Logistic Regression ‚úÖ

Naive Bayes üß†

Support Vector Machine (SVM) üíª

Random Forest üå≤

Outputs accuracy and classification reports for each

Visualizes a confusion matrix for each
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("/content/Cleaned_Merged_All_Data.csv")
df = df.dropna(subset=["review_text", "sentiment_label"])

# Encode target labels
le = LabelEncoder()
df["sentiment_encoded"] = le.fit_transform(df["sentiment_label"])

# Optional: use instructor/category as extra text features
df["instructor_id"] = df["instructor_id"].astype(str)
df["category"] = df["category"].astype(str)
df["full_text"] = df["review_text"] + " category_" + df["category"] + " instructor_" + df["instructor_id"]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(df["full_text"], df["sentiment_encoded"], test_size=0.2, random_state=42)

# Define model candidates
models = {
    "Logistic Regression": LogisticRegression(max_iter=200),
    "Naive Bayes": MultinomialNB(),
    "SVM": LinearSVC(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

# Store results
results = {}

# Loop through models
for name, model in models.items():
    print(f"üîç Training: {name}")

    pipeline = make_pipeline(
        TfidfVectorizer(stop_words="english", max_features=3000),
        model
    )
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    print(f"‚úÖ Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    # Save result
    results[name] = {
        "model": pipeline,
        "accuracy": acc,
        "confusion_matrix": confusion_matrix(y_test, y_pred)
    }

    # Plot confusion matrix
    plt.figure(figsize=(5, 4))
    sns.heatmap(results[name]["confusion_matrix"], annot=True, fmt="d", cmap="YlGnBu",
                xticklabels=le.classes_, yticklabels=le.classes_)
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.show()

"""‚úÖ Full Evaluation Metrics

Metric	Why It‚Äôs Useful

‚úÖ Accuracy	General performance across all classes

‚úÖ Precision, Recall, F1	Especially important for imbalanced classes like "Negative"

‚úÖ Confusion Matrix	Shows true vs. predicted breakdown

‚úÖ ROC-AUC	Great for visualizing class separation (works best with
 2-class but possible for multi-class too)

"""

from sklearn.metrics import roc_auc_score, RocCurveDisplay
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np

# Re-binarize for ROC-AUC (multiclass)
Y_bin = label_binarize(y_test, classes=[0, 1, 2])
n_classes = Y_bin.shape[1]

# Use best performing model or Logistic Regression here
best_model = models["Logistic Regression"]
pipeline = make_pipeline(
    TfidfVectorizer(stop_words="english", max_features=3000),
    OneVsRestClassifier(LogisticRegression(max_iter=200))
)
pipeline.fit(X_train, y_train)
y_score = pipeline.predict_proba(X_test)

# Calculate ROC-AUC for each class
roc_auc = {}
for i in range(n_classes):
    roc_auc[le.classes_[i]] = roc_auc_score(Y_bin[:, i], y_score[:, i])

# Display results
print("üéØ ROC-AUC by Class:")
for label, score in roc_auc.items():
    print(f"{label}: {score:.3f}")

# Optional: Plot ROC curve for each class
plt.figure(figsize=(8, 6))
for i in range(n_classes):
    RocCurveDisplay.from_predictions(Y_bin[:, i], y_score[:, i], name=le.classes_[i])

plt.title("ROC Curves for Sentiment Classes")
plt.plot([0, 1], [0, 1], 'k--', label='No Skill')
plt.legend()
plt.tight_layout()
plt.show()

"""#Perfect! A side-by-side comparison table of all model metrics will help you:

‚úÖ See which model performs best overall

‚úÖ Compare precision, recall, and F1-score for the Negative class

‚úÖ Decide which model to use in production or reporting

‚úÖ üì¶ Add This Block at the End of Your Colab Notebook:
# New section








"""

from sklearn.metrics import precision_score, recall_score, f1_score

# Initialize results summary table
comparison = []

# Loop through each model again to collect metrics
for name, result in results.items():
    y_pred = result["model"].predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average=None, zero_division=0)
    rec = recall_score(y_test, y_pred, average=None, zero_division=0)
    f1 = f1_score(y_test, y_pred, average=None, zero_division=0)

    comparison.append({
        "Model": name,
        "Accuracy": acc,
        "Negative Precision": prec[0],
        "Negative Recall": rec[0],
        "Negative F1": f1[0],
        "Neutral F1": f1[1],
        "Positive F1": f1[2]
    })

# Convert to DataFrame
metrics_df = pd.DataFrame(comparison)

# Format table for readability
metrics_df = metrics_df.sort_values(by="Negative F1", ascending=False).round(3)
display(metrics_df)

# Optional: Bar plot for visual comparison
metrics_df.plot(
    x="Model",
    y=["Accuracy", "Negative Precision", "Negative Recall", "Negative F1"],
    kind="bar",
    figsize=(12, 6),
    title="Model Comparison on Key Metrics",
    colormap="Set2"
)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""#PHASE 7: STRUCTURING THE OUTPUT TABLE"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
import numpy as np

# Reload cleaned dataset
df = pd.read_csv("/content/Cleaned_Merged_All_Data.csv")
df = df.dropna(subset=["review_text"])

# Use only text and metadata
df["instructor_id"] = df["instructor_id"].astype(str)
df["category"] = df["category"].astype(str)
df["full_text"] = df["review_text"] + " category_" + df["category"] + " instructor_" + df["instructor_id"]

# Use best trained model
final_model = results["Logistic Regression"]["model"]
vectorizer = final_model.named_steps["tfidfvectorizer"]
classifier = final_model.named_steps["logisticregression"]

# TF-IDF transform
X_vectorized = vectorizer.transform(df["full_text"])

# Get predictions
sentiment_preds = classifier.predict(X_vectorized)
probas = classifier.predict_proba(X_vectorized)
confidence = np.max(probas, axis=1)

# Scale the sentiment score to -1 (negative), 0 (neutral), 1 (positive)
sentiment_map = {0: -1, 1: 0, 2: 1}
sentiment_score = [sentiment_map[p] for p in sentiment_preds]
sentiment_label = le.inverse_transform(sentiment_preds)

# Extract keywords
top_keywords = vectorizer.inverse_transform(X_vectorized)
keyword_tags = ["; ".join(words[:5]) for words in top_keywords]

# OPTIONAL: Dummy placeholder for topic modeling
dominant_topic = ["pace" if "fast" in text.lower() else "clarity" for text in df["review_text"]]

# Build structured output DataFrame
structured_output = pd.DataFrame({
    "review_id": df["review_id"],
    "sentiment_score": sentiment_score,
    "sentiment_label": sentiment_label,
    "dominant_topic": dominant_topic,
    "keyword_tags": keyword_tags,
    "confidence_level": confidence.round(3)
})

# Export to CSV
structured_output_path = "/content/Structured_Sentiment_Output.csv"
structured_output.to_csv(structured_output_path, index=False)

# Optional: Download the file
from google.colab import files
files.download(structured_output_path)















